"""
Chunking Strategyå®éªŒä¸»ç¨‹åº
ç ”ç©¶ä¸åŒæ–‡æœ¬åˆ†å—ç­–ç•¥å¯¹RAGæ£€ç´¢æ•ˆæœçš„å½±å“
"""

import os
import yaml
import json
import time
from datetime import datetime
from typing import List, Dict
import numpy as np
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.embeddings import EmbeddingModelFactory
from src.rag_system import RAGSystem, TextChunker
from src.data_loader import load_squad_data
from src.evaluator import RAGEvaluator


def load_config(config_path: str = "config/chunking_config.yaml") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def run_chunking_experiment(config: Dict, documents: List[Dict], test_queries: List[Dict],
                            chunk_size: int, overlap: int, strategy: str = "fixed", 
                            output_dir: str = None) -> Dict:
    """
    è¿è¡Œå•ä¸ªchunkingå®éªŒ
    
    Args:
        config: é…ç½®å­—å…¸
        passages: æ–‡æ¡£åˆ—è¡¨
        test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        chunk_size: chunkå¤§å°
        overlap: é‡å å¤§å°
        strategy: chunkingç­–ç•¥
        output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜è¯„ä¼°æ•°æ®é›†ï¼‰
        
    Returns:
        å®éªŒç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"å®éªŒ: {strategy} | Size={chunk_size} | Overlap={overlap}")
    print(f"{'='*80}")
    
    # 1. åˆ›å»ºembeddingæ¨¡å‹(å›ºå®šä½¿ç”¨qwen3-0.6b)
    model_config = config['embedding_model']
    # embedding_model = EmbeddingModelFactory.create_model(
    #     model_type=model_config['type'],
    #     model_id=model_config['model_id'],
    #     model_name=model_config['name']
    # )

    embedding_model = EmbeddingModelFactory.create_model(
        config=model_config
    )
    
    # 2. æ–‡æ¡£åˆ†å—
    print(f"\n[1/4] æ–‡æ¡£åˆ†å—...")
    start_time = time.time()
    
    if strategy == "fixed":
        chunked_docs = TextChunker.chunk_documents(
            documents, 
            chunk_size=chunk_size, 
            overlap=overlap,
            strategy="fixed"
        )
    elif strategy == "sentence":
        chunked_docs = TextChunker.chunk_documents(
            documents,
            chunk_size=chunk_size,
            overlap=0,  # sentenceç­–ç•¥ä¸ä½¿ç”¨overlap
            strategy="sentence"
        )
    elif strategy == "semantic":
        chunked_docs = TextChunker.chunk_documents(
            documents,
            chunk_size=chunk_size,
            overlap=overlap,
            strategy="semantic",
            embedding_model=embedding_model  # è¯­ä¹‰åˆ†å—éœ€è¦embeddingæ¨¡å‹
        )
    elif strategy == "recursive":
        chunked_docs = TextChunker.chunk_documents(
            documents,
            chunk_size=chunk_size,
            overlap=overlap,
            strategy="recursive"
        )
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    chunk_time = time.time() - start_time
    print(f"  å®Œæˆåˆ†å—: {len(documents)} æ–‡æ¡£ â†’ {len(chunked_docs)} chunks ({chunk_time:.2f}s)")
    print(f"  å¹³å‡chunké•¿åº¦: {np.mean([len(c['text']) for c in chunked_docs]):.1f} å­—ç¬¦")
    
    # 3. æ„å»ºRAGç³»ç»Ÿå¹¶ç´¢å¼•
    print(f"\n[2/4] æ„å»ºå‘é‡ç´¢å¼•...")
    rag_system = RAGSystem(embedding_model, config)
    index_time = rag_system.index_documents(chunked_docs)
    
    # 4. è¯„ä¼°
    print(f"\n[3/4] è¯„ä¼°æ£€ç´¢æ€§èƒ½...")
    evaluator = RAGEvaluator(config)
    
    # è®¾ç½®è¾“å‡ºç›®å½•å’Œchunké…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
    if output_dir:
        evaluator.set_output_dir(output_dir)
    evaluator.set_chunk_config(chunk_size, overlap)
    
    eval_results = evaluator.evaluate_retrieval(rag_system, test_queries, chunked_docs)
    
    # 5. æ±‡æ€»ç»“æœ
    results = {
        'experiment_config': {
            'strategy': strategy,
            'chunk_size': chunk_size,
            'overlap': overlap,
            'model': model_config['name']
        },
        'chunking_stats': {
            'num_original_docs': len(documents),
            'num_chunks': len(chunked_docs),
            'avg_chunk_length': float(np.mean([len(c['text']) for c in chunked_docs])),
            'chunk_time': chunk_time
        },
        'index_build_time': index_time,
        'metrics': eval_results['metrics']
    }
    
    # æ‰“å°å…³é”®æŒ‡æ ‡
    print(f"\n[4/4] ç»“æœæ‘˜è¦:")
    print(f"  Chunk Size: {chunk_size} | Overlap: {overlap}")
    print(f"  Chunks: {len(chunked_docs)}")
    print(f"  Context Precision: {results['metrics'].get('context_precision', 0):.4f}")
    # print(f"  Context Recall: {results['metrics'].get('context_recall', 0):.4f}")
    print(f"  æ£€ç´¢æ—¶é—´: {results['metrics'].get('avg_retrieval_time', 0):.4f}s")
    
    return results


def run_all_experiments(config: Dict) -> List[Dict]:
    """è¿è¡Œæ‰€æœ‰å®éªŒ"""
    
    # åŠ è½½æ•°æ®
    print("\n" + "="*80)
    print("åŠ è½½SQuAD v2æ•°æ®é›†...")
    print("="*80)
    
    documents, train_queries, test_queries = load_squad_data(config)
    
    print(f"\næ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  Documents: {len(documents)}")
    print(f"  Test Queries: {len(test_queries)}")
    
    # åˆ›å»ºä¸»è¾“å‡ºç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    test_size = config['evaluation']['test_size']
    base_output_dir = config['output']['results_dir']
    main_output_dir = os.path.join(base_output_dir, f"{timestamp}_s{test_size}")
    os.makedirs(main_output_dir, exist_ok=True)
    print(f"\nğŸ“ ä¸»è¾“å‡ºç›®å½•: {main_output_dir}")
    
    # è¿è¡Œå®éªŒ
    all_results = []
    
    for strategy_config in config['chunking_strategies']:
        strategy_name = strategy_config['name']
        
        # è·³è¿‡semanticç­–ç•¥(å¦‚æœé…ç½®è¦æ±‚)
        if strategy_name == "semantic_based" and config['experiment'].get('skip_semantic', True):
            print(f"\nè·³è¿‡ {strategy_name} ç­–ç•¥å®éªŒ(é…ç½®è¦æ±‚)")
            continue
        
        # åªè¿è¡Œbaselineå®éªŒ(å¦‚æœé…ç½®è¦æ±‚)
        if config['experiment'].get('run_baseline_only', False) and strategy_name != "fixed_size":
            print(f"\nè·³è¿‡ {strategy_name} ç­–ç•¥å®éªŒ(ä»…è¿è¡Œbaseline)")
            continue
        
        for exp_config in strategy_config['experiments']:
            # æ£€æŸ¥å®éªŒæ•°é‡é™åˆ¶
            if len(all_results) >= config['experiment'].get('max_experiments', 20):
                print(f"\nè¾¾åˆ°æœ€å¤§å®éªŒæ•°é‡é™åˆ¶({config['experiment']['max_experiments']})")
                break
            
            # è¿è¡Œå®éªŒ
            if strategy_name in ["fixed_size", "fixed"]:
                result = run_chunking_experiment(
                    config, documents, test_queries,
                    chunk_size=exp_config['chunk_size'],
                    overlap=exp_config['overlap'],
                    strategy="fixed",
                    output_dir=main_output_dir
                )
            elif strategy_name == "sentence_based":
                result = run_chunking_experiment(
                    config, documents, test_queries,
                    chunk_size=exp_config['target_size'],
                    overlap=0,
                    strategy="sentence",
                    output_dir=main_output_dir
                )
            elif strategy_name == "semantic_based":
                result = run_chunking_experiment(
                    config, documents, test_queries,
                    chunk_size=exp_config['target_size'],
                    overlap=exp_config.get('overlap', 0),
                    strategy="semantic",
                    output_dir=main_output_dir
                )
            elif strategy_name == "recursive_based":
                result = run_chunking_experiment(
                    config, documents, test_queries,
                    chunk_size=exp_config['target_size'],
                    overlap=exp_config.get('overlap', 0),
                    strategy="recursive",
                    output_dir=main_output_dir
                )
            else:
                continue
            
            all_results.append(result)
    
    return all_results, main_output_dir  # è¿”å›ç»“æœå’Œè¾“å‡ºç›®å½•


def save_results(results: List[Dict], output_dir: str):
    """ä¿å­˜å®éªŒç»“æœ"""
    
    print(f"\nğŸ“ ç»“æœä¿å­˜åˆ°: {output_dir}")
    
    # ä¿å­˜JSONç»“æœ
    json_file = os.path.join(output_dir, "results.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¯¦ç»†ç»“æœ: results.json")
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_file = os.path.join(output_dir, "report.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("Chunking Strategy å®éªŒæŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»å®éªŒæ•°: {len(results)}\n\n")
        
        f.write("å®éªŒç»“æœ:\n")
        f.write("-"*80 + "\n\n")
        
        for i, result in enumerate(results, 1):
            exp = result['experiment_config']
            metrics = result['metrics']
            
            f.write(f"{i}. {exp['strategy']} | Size={exp['chunk_size']} | Overlap={exp['overlap']}\n")
            f.write(f"   Chunks: {result['chunking_stats']['num_chunks']}\n")
            f.write(f"   Context Precision: {metrics.get('context_precision', 0):.4f}\n")
            
            # æ‰“å°è¯¦ç»†precisionå€¼
            if 'context_precision_list' in metrics:
                precision_list = metrics['context_precision_list']
                f.write(f"     è¯¦ç»†å€¼ ({len(precision_list)}æ¡): {[f'{x:.4f}' for x in precision_list]}\n")
            
            # f.write(f"   Context Recall: {metrics.get('context_recall', 0):.4f}\n")
            # 
            # # æ‰“å°è¯¦ç»†recallå€¼
            # if 'context_recall_list' in metrics:
            #     recall_list = metrics['context_recall_list']
            #     f.write(f"     è¯¦ç»†å€¼ ({len(recall_list)}æ¡): {[f'{x:.4f}' for x in recall_list]}\n")
            
            f.write(f"   æ£€ç´¢æ—¶é—´: {metrics.get('avg_retrieval_time', 0):.4f}s\n")
            
            f.write("\n")
    
    print(f"âœ“ æ–‡æœ¬æŠ¥å‘Š: report.txt")


def generate_visualizations(results: List[Dict], output_dir: str):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # # æå–fixedç­–ç•¥çš„ç»“æœ(ç”¨äºçƒ­åŠ›å›¾)
        # fixed_results = [r for r in results if r['experiment_config']['strategy'] == 'fixed']
        
        # if not fixed_results:
        #     print("æ²¡æœ‰fixedç­–ç•¥ç»“æœ,è·³è¿‡çƒ­åŠ›å›¾")
        # else:
        #     # å‡†å¤‡æ•°æ®
        #     sizes = sorted(set(r['experiment_config']['chunk_size'] for r in fixed_results))
        #     overlaps = sorted(set(r['experiment_config']['overlap'] for r in fixed_results))
        #     
        #     precision_data = np.zeros((len(overlaps), len(sizes)))
        #     recall_data = np.zeros((len(overlaps), len(sizes)))
        #     
        #     for r in fixed_results:
        #         size_idx = sizes.index(r['experiment_config']['chunk_size'])
        #         overlap_idx = overlaps.index(r['experiment_config']['overlap'])
        #         precision_data[overlap_idx][size_idx] = r['metrics'].get('context_precision', 0)
        #         recall_data[overlap_idx][size_idx] = r['metrics'].get('context_recall', 0)
        #     
        #     # 1. Context Precisionçƒ­åŠ›å›¾
        #     plt.figure(figsize=(10, 6))
        #     sns.heatmap(precision_data, annot=True, fmt='.3f', 
        #                xticklabels=sizes, yticklabels=overlaps,
        #                cmap='YlOrRd', cbar_kws={'label': 'Context Precision'})
        #     plt.xlabel('Chunk Size')
        #     plt.ylabel('Overlap')
        #     plt.title('Context Precision: Chunk Size vs Overlap (Fixed Strategy)')
        #     
        #     precision_file = os.path.join(output_dir, "heatmap_precision.png")
        #     plt.savefig(precision_file, dpi=300, bbox_inches='tight')
        #     plt.close()
        #     print(f"âœ“ Context Precisionçƒ­åŠ›å›¾: heatmap_precision.png")
        #     
        #     # 2. Context Recallçƒ­åŠ›å›¾
        #     plt.figure(figsize=(10, 6))
        #     sns.heatmap(recall_data, annot=True, fmt='.3f', 
        #                xticklabels=sizes, yticklabels=overlaps,
        #                cmap='YlGnBu', cbar_kws={'label': 'Context Recall'})
        #     plt.xlabel('Chunk Size')
        #     plt.ylabel('Overlap')
        #     plt.title('Context Recall: Chunk Size vs Overlap (Fixed Strategy)')
        #     
        #     recall_file = os.path.join(output_dir, "heatmap_recall.png")
        #     plt.savefig(recall_file, dpi=300, bbox_inches='tight')
        #     plt.close()
        #     print(f"âœ“ Context Recallçƒ­åŠ›å›¾: heatmap_recall.png")
        
        # 3. Context PrecisionæŠ˜çº¿å›¾ - æ‰€æœ‰ç­–ç•¥
        plt.figure(figsize=(16, 6))
        
        # ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºæ ‡ç­¾ï¼ˆåŒ…å«æ‰€æœ‰ç­–ç•¥ï¼‰
        x_labels = []
        for r in results:
            exp = r['experiment_config']
            strategy = exp['strategy']
            chunk_size = exp['chunk_size']
            overlap = exp['overlap']
            
            if strategy == 'fixed':
                x_labels.append(f"fixed\n({chunk_size},{overlap})")
            elif strategy == 'sentence':
                x_labels.append(f"sentence\n(size:{chunk_size})")
            elif strategy == 'semantic':
                x_labels.append(f"semantic\n({chunk_size},{overlap})")
            elif strategy == 'recursive':
                x_labels.append(f"recursive\n({chunk_size},{overlap})")
            else:
                x_labels.append(f"{strategy}\n({chunk_size},{overlap})")
        
        x_positions = list(range(len(results)))
        precision_vals = [r['metrics'].get('context_precision', 0) for r in results]
        
        # ç»˜åˆ¶æŠ˜çº¿ï¼Œä¸åŒç­–ç•¥ç”¨ä¸åŒé¢œè‰²
        colors = []
        for r in results:
            strategy = r['experiment_config']['strategy']
            if strategy == 'fixed':
                colors.append('orangered')
            elif strategy == 'sentence':
                colors.append('steelblue')
            elif strategy == 'semantic':
                colors.append('green')
            elif strategy == 'recursive':
                colors.append('purple')
            else:
                colors.append('gray')
        
        plt.plot(x_positions, precision_vals, marker='o', linewidth=2, markersize=8, color='darkgray', alpha=0.5)
        plt.scatter(x_positions, precision_vals, c=colors, s=100, zorder=3)
        
        # åœ¨æ¯ä¸ªç‚¹æ—è¾¹æ ‡æ³¨æ•°å€¼
        for i, (x, y) in enumerate(zip(x_positions, precision_vals)):
            plt.annotate(f'{y:.3f}', xy=(x, y), xytext=(0, 10), 
                       textcoords='offset points', fontsize=9, ha='center',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        
        plt.xlabel('(Strategy, Size, Overlap)', fontsize=12)
        plt.ylabel('Context Precision', fontsize=12)
        plt.title('Context Precision Across All Experiments', fontsize=14)
        plt.xticks(x_positions, x_labels, rotation=45, ha='right', fontsize=9)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='orangered', label='Fixed'),
            Patch(facecolor='steelblue', label='Sentence'),
            Patch(facecolor='green', label='Semantic'),
            Patch(facecolor='purple', label='Recursive')
        ]
        plt.legend(handles=legend_elements, loc='upper left')
        plt.tight_layout()
        
        line_precision_file = os.path.join(output_dir, "line_precision.png")
        plt.savefig(line_precision_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ Context PrecisionæŠ˜çº¿å›¾: line_precision.png")
        
        # # 4. Context RecallæŠ˜çº¿å›¾ - æ‰€æœ‰ç­–ç•¥
        # plt.figure(figsize=(16, 6))
        # 
        # recall_vals = [r['metrics'].get('context_recall', 0) for r in results]
        # 
        # plt.plot(x_positions, recall_vals, marker='s', linewidth=2, markersize=8, color='darkgray', alpha=0.5)
        # plt.scatter(x_positions, recall_vals, c=colors, s=100, zorder=3)
        # 
        # # åœ¨æ¯ä¸ªç‚¹æ—è¾¹æ ‡æ³¨æ•°å€¼
        # for i, (x, y) in enumerate(zip(x_positions, recall_vals)):
        #     plt.annotate(f'{y:.3f}', xy=(x, y), xytext=(0, 10), 
        #                textcoords='offset points', fontsize=9, ha='center',
        #                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))
        # 
        # plt.xlabel('(Strategy, Size, Overlap)', fontsize=12)
        # plt.ylabel('Context Recall', fontsize=12)
        # plt.title('Context Recall Across All Experiments', fontsize=14)
        # plt.xticks(x_positions, x_labels, rotation=45, ha='right', fontsize=9)
        # plt.grid(True, alpha=0.3)
        # plt.legend(handles=legend_elements, loc='upper left')
        # plt.tight_layout()
        # 
        # line_recall_file = os.path.join(output_dir, "line_recall.png")
        # plt.savefig(line_recall_file, dpi=300, bbox_inches='tight')
        # plt.close()
        # print(f"âœ“ Context RecallæŠ˜çº¿å›¾: line_recall.png")
        
    except ImportError:
        print("matplotlibæœªå®‰è£…,è·³è¿‡å¯è§†åŒ–")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("Chunking Strategy å¯¹ RAG æ£€ç´¢æ•ˆæœçš„å½±å“å®éªŒ")
    print("="*80)
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è¿è¡Œå®éªŒ
    results, output_dir = run_all_experiments(config)
    
    # ä¿å­˜ç»“æœ
    save_results(results, output_dir)
    
    # ç”Ÿæˆå¯è§†åŒ–
    if config['output'].get('generate_plots', True):
        generate_visualizations(results, output_dir)
    
    print("\n" + "="*80)
    print(f"å®éªŒå®Œæˆ! å…±è¿è¡Œ {len(results)} ä¸ªå®éªŒ")
    print("="*80)


if __name__ == "__main__":
    main()
