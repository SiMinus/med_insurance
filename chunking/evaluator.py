"""
è¯„ä¼°æ¨¡å—
ä½¿ç”¨Ragasåº“è¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡
"""

import time
import os
from typing import List, Dict
import numpy as np
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import context_precision, context_recall
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_community.embeddings.dashscope import DashScopeEmbeddings
import logging

# ç¦ç”¨tokenizersè­¦å‘Šå’Œragasæ—¥å¿—
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
logging.getLogger('ragas').setLevel(logging.WARNING)
logging.getLogger('datasets').setLevel(logging.WARNING)


class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨ - åŸºäºRagas"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.metrics = config['evaluation']['metrics']
        self.output_dir = None  # ç”¨äºä¿å­˜è¯„ä¼°æ•°æ®é›†
        self.chunk_size = None  # å½“å‰å®éªŒçš„chunk_size
        self.overlap = None  # å½“å‰å®éªŒçš„overlap
        
    def set_output_dir(self, output_dir: str):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_dir = output_dir
    
    def set_chunk_config(self, chunk_size: int, overlap: int):
        """è®¾ç½®chunké…ç½®"""
        self.chunk_size = chunk_size
        self.overlap = overlap
        
    def evaluate_retrieval(self, rag_system, test_qa_pairs: List[Dict], chunked_docs: List[Dict] = None) -> Dict:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½ - ä½¿ç”¨RagasæŒ‡æ ‡
        
        Args:
            rag_system: RAGç³»ç»Ÿå®ä¾‹
            test_qa_pairs: æµ‹è¯•é—®ç­”å¯¹åˆ—è¡¨ [{'question': ..., 'answers': [...], 'doc_id': ..., 'context': ...}]
            chunked_docs: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨(å¯é€‰,ç”¨äºç»Ÿè®¡)
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nå¼€å§‹è¯„ä¼° (æ¨¡å‹: {rag_system.embedding_model.model_name})...")
        
        results = {
            'model_name': rag_system.embedding_model.model_name,
            'model_id': rag_system.embedding_model.model_id,
            'metrics': {}
        }
        
        # 1. æ”¶é›†æ£€ç´¢ç»“æœå’Œæ£€ç´¢æ—¶é—´
        questions = []
        ground_truths = []
        contexts_list = []
        retrieval_times = []
        
        for qa in test_qa_pairs:
            question = qa['question']
            answers = qa.get('answers', [])
            
            # è·³è¿‡æ— ç­”æ¡ˆçš„é—®é¢˜(SQuAD v2)
            if not answers:
                continue
            
            # æ‰§è¡Œæ£€ç´¢
            retrieved_docs, retrieval_time = rag_system.retrieve(question)
            retrieval_times.append(retrieval_time)
            
            # å‡†å¤‡Ragasè¯„ä¼°æ•°æ®
            questions.append(question)
            # RagasæœŸæœ›ground_truthæ˜¯å­—ç¬¦ä¸²,ä¸æ˜¯åˆ—è¡¨,å°†æ‰€æœ‰ç­”æ¡ˆç”¨' or 'è¿æ¥
            ground_truths.append(" or ".join(answers) if answers else "")
            contexts_list.append([doc['text'] for doc in retrieved_docs])  # æ£€ç´¢åˆ°çš„contexts
        
        print(f"  æœ‰æ•ˆé—®é¢˜æ•°: {len(questions)}")
        
        # 2. è®¡ç®—æ£€ç´¢æ—¶é—´æŒ‡æ ‡
        if retrieval_times:
            results['metrics']['avg_retrieval_time'] = float(np.mean(retrieval_times))
            results['metrics']['total_retrieval_time'] = float(np.sum(retrieval_times))
        
        # 3. ä½¿ç”¨Ragasè¯„ä¼°context precisionå’Œcontext recall
        if len(questions) > 0:
            try:
                # æ„å»ºRagasæ•°æ®é›†
                eval_dataset = Dataset.from_dict({
                    'question': questions,
                    'ground_truth': ground_truths,
                    'contexts': contexts_list
                })
                
                # å…ˆæ‰§è¡ŒRagasè¯„ä¼°è·å–æŒ‡æ ‡å€¼ï¼ˆç§»åˆ°ä¿å­˜CSVä¹‹å‰ï¼‰
                ragas_metrics = []
                if 'context_precision' in self.metrics:
                    ragas_metrics.append(context_precision)
                # if 'context_recall' in self.metrics:
                #     ragas_metrics.append(context_recall)
                
                # ç”¨äºä¿å­˜åˆ°CSVçš„åˆ—è¡¨
                precision_list = []
                # recall_list = []
                
                if ragas_metrics:
                    # æ‰§è¡ŒRagasè¯„ä¼°
                    print(f"  æ‰§è¡ŒRagasè¯„ä¼°...")
                    custom_llm = ChatTongyi(
                        model_name='qwen-plus',
                        temperature=0,
                        request_timeout=120,
                    )

                    embeddings = DashScopeEmbeddings()
                    ragas_result = evaluate(
                        dataset=eval_dataset,
                        metrics=ragas_metrics,
                        llm=custom_llm,
                        embeddings=embeddings
                    )
                    
                    # æå–è¯„ä¼°ç»“æœå¹¶è®¡ç®—å‡å€¼
                    if hasattr(ragas_result, 'to_pandas'):
                        df_ragas = ragas_result.to_pandas()
                        print(f"  ğŸ“Š Ragas DataFrame shape: {df_ragas.shape}")
                        print(f"  ğŸ“Š Columns: {df_ragas.columns.tolist()}")
                        
                        # å¯¹æ¯ä¸ªæŒ‡æ ‡å–å¹³å‡å€¼
                        if 'context_precision' in self.metrics and 'context_precision' in df_ragas.columns:
                            precision_series = df_ragas['context_precision']
                            precision_val = precision_series.mean()
                            results['metrics']['context_precision'] = float(precision_val) if not np.isnan(precision_val) else 0.0
                            results['metrics']['context_precision_list'] = precision_series.tolist()  # ä¿å­˜å®Œæ•´åˆ—è¡¨
                            precision_list = precision_series.tolist()  # ç”¨äºCSV
                            print(f"  âœ“ Context Precision: {results['metrics']['context_precision']:.4f}")
                            print(f"     è¯¦ç»†å€¼: {[f'{x:.4f}' for x in precision_series.tolist()]}")
                            
                        # if 'context_recall' in self.metrics and 'context_recall' in df_ragas.columns:
                        #     recall_series = df_ragas['context_recall']
                        #     recall_val = recall_series.mean()
                        #     results['metrics']['context_recall'] = float(recall_val) if not np.isnan(recall_val) else 0.0
                        #     results['metrics']['context_recall_list'] = recall_series.tolist()  # ä¿å­˜å®Œæ•´åˆ—è¡¨
                        #     recall_list = recall_series.tolist()  # ç”¨äºCSV
                        #     print(f"  âœ“ Context Recall: {results['metrics']['context_recall']:.4f}")
                        #     print(f"     è¯¦ç»†å€¼: {[f'{x:.4f}' for x in recall_series.tolist()]}")
                        
                        print(f"  âœ“ Ragasè¯„ä¼°å®Œæˆ")
                
                # ä¿å­˜è¯„ä¼°æ•°æ®é›†åˆ°CSVï¼ˆåŒ…å«RagasæŒ‡æ ‡ï¼‰
                if self.output_dir:
                    try:
                        # è½¬æ¢contextsåˆ—è¡¨ä¸ºå­—ç¬¦ä¸²ï¼ˆå› ä¸ºCSVä¸æ”¯æŒåˆ—è¡¨ï¼‰
                        csv_data = []
                        for i in range(len(questions)):
                            csv_data.append({
                                'question': questions[i],
                                'ground_truth': ground_truths[i],
                                'contexts': '\n\n'.join(contexts_list[i]),  # ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦åˆ†éš”
                                'context_precision': precision_list[i] if precision_list[i] is not None else '',
                                'chunk_size': self.chunk_size,
                                'overlap': self.overlap,
                                'num_contexts': len(contexts_list[i]),
                                # 'context_recall': recall_list[i] if recall_list[i] is not None else '',
                            })
                        
                        df_csv = pd.DataFrame(csv_data)
                        csv_path = os.path.join(self.output_dir, 'ragas_eval_dataset.csv')
                        
                        # è¿½åŠ å†™å…¥æ¨¡å¼ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
                        if os.path.exists(csv_path):
                            df_csv.to_csv(csv_path, mode='a', header=False, index=False, encoding='utf-8-sig')
                        else:
                            df_csv.to_csv(csv_path, index=False, encoding='utf-8-sig')
                        
                        print(f"  ğŸ’¾ è¯„ä¼°æ•°æ®é›†å·²ä¿å­˜: ragas_eval_dataset.csv (chunk_size={self.chunk_size}, overlap={self.overlap}, {len(questions)} æ¡)")
                    except Exception as e:
                        print(f"  âš ï¸  ä¿å­˜CSVå¤±è´¥: {e}")
            
            except Exception as e:
                print(f"  âš ï¸  Ragasè¯„ä¼°å‡ºé”™: {e}")
                # å¦‚æœRagasè¯„ä¼°å¤±è´¥,ä½¿ç”¨å¤‡ç”¨ç®€å•è®¡ç®—
                results['metrics']['context_precision'] = self._fallback_context_precision(
                    test_qa_pairs, rag_system
                )
                results['metrics']['context_recall'] = self._fallback_context_recall(
                    test_qa_pairs, rag_system
                )
        
        # 4. æ·»åŠ åˆ†å—ç»Ÿè®¡ä¿¡æ¯(å¦‚æœæä¾›)
        if chunked_docs:
            results['metrics']['num_chunks'] = len(chunked_docs)
            results['metrics']['avg_chunk_length'] = float(np.mean([len(c['text']) for c in chunked_docs]))
        
        return results
    
    def _fallback_context_precision(self, test_qa_pairs: List[Dict], rag_system) -> float:
        """
        å¤‡ç”¨Context Precisionè®¡ç®—(ç®€åŒ–ç‰ˆ)
        æ£€æŸ¥æ£€ç´¢åˆ°çš„chunkä¸­æœ‰å¤šå°‘åŒ…å«ç­”æ¡ˆ
        """
        total_retrieved = 0
        relevant_retrieved = 0
        
        for qa in test_qa_pairs:
            if not qa.get('answers'):
                continue
            
            question = qa['question']
            answers = qa['answers']
            
            retrieved_docs, _ = rag_system.retrieve(question)
            
            for doc in retrieved_docs:
                total_retrieved += 1
                # æ£€æŸ¥chunkä¸­æ˜¯å¦åŒ…å«ä»»æ„ç­”æ¡ˆ
                doc_text = doc['text'].lower()
                if any(ans.lower() in doc_text for ans in answers if ans):
                    relevant_retrieved += 1
        
        return relevant_retrieved / total_retrieved if total_retrieved > 0 else 0.0
    
    def _fallback_context_recall(self, test_qa_pairs: List[Dict], rag_system) -> float:
        """
        å¤‡ç”¨Context Recallè®¡ç®—(ç®€åŒ–ç‰ˆ)
        æ£€æŸ¥æœ‰å¤šå°‘é—®é¢˜çš„ç­”æ¡ˆè¢«æ£€ç´¢åˆ°
        """
        found_count = 0
        total_count = 0
        
        for qa in test_qa_pairs:
            if not qa.get('answers'):
                continue
            
            total_count += 1
            question = qa['question']
            answers = qa['answers']
            
            retrieved_docs, _ = rag_system.retrieve(question)
            
            # æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°åŒ…å«ç­”æ¡ˆçš„chunk
            for doc in retrieved_docs:
                doc_text = doc['text'].lower()
                if any(ans.lower() in doc_text for ans in answers if ans):
                    found_count += 1
                    break
        
        return found_count / total_count if total_count > 0 else 0.0
        # return found_count / total_count if total_count > 0 else 0.0


if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°å™¨
    config = {
        'evaluation': {
            'metrics': ['context_precision', 'context_recall', 'retrieval_time']
        }
    }
    
    evaluator = RAGEvaluator(config)
    print("RAG Evaluator (Ragasç‰ˆæœ¬) åˆå§‹åŒ–æˆåŠŸ")
