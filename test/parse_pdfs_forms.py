#!/usr/bin/env python3
"""
PDF æ–‡æœ¬æå–è„šæœ¬ - ä½¿ç”¨ Unstructured åº“è‡ªåŠ¨å¤„ç†

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ unstructured åº“è‡ªåŠ¨åˆ¤æ–­ PDF ç±»å‹
2. è‡ªåŠ¨å†³å®šæ˜¯å¦éœ€è¦ OCR
3. æ”¯æŒæ–‡æœ¬å‹å’Œå›¾åƒå‹ PDF
4. è¾“å‡ºä¸º documents_example.json æ ¼å¼

ç”¨æ³•ï¼š
ç›´æ¥ä¿®æ”¹ main() å‡½æ•°ä¸­çš„å‚æ•°é…ç½®åè¿è¡Œï¼š
python test/parse_pdfs.py
"""

import json
import sys
from pathlib import Path
from typing import List, Dict
import re


def extract_with_pypdf(pdf_path: Path) -> str:
    """ä½¿ç”¨ pypdf æå–æ–‡æœ¬"""
    try:
        from pypdf import PdfReader
        reader = PdfReader(str(pdf_path))
        text_parts = []
        for page in reader.pages:
            text = page.extract_text()
            if text:
                text_parts.append(text)
        return "\n".join(text_parts)
    except Exception as e:
        print(f"  pypdf å¤±è´¥: {e}")
        return ""


def extract_with_unstructured(pdf_path: Path) -> str:
    """ä½¿ç”¨ unstructured æå–æ–‡æœ¬ï¼ˆä½¿ç”¨ OCRï¼‰"""
    try:
        from unstructured.partition.pdf import partition_pdf
        elements = partition_pdf(
            filename=str(pdf_path),
            strategy="hi_res",  # é«˜ç²¾åº¦æ¨¡å¼ï¼Œä½¿ç”¨ OCR
            languages=["chi_sim", "eng"],
        )
        text_parts = [element.text for element in elements if hasattr(element, 'text') and element.text]
        return "\n".join(text_parts)
    except Exception as e:
        print(f"  unstructured å¤±è´¥: {e}")
        return ""


def is_valid_text(text: str, min_length: int = 100, min_chinese_ratio: float = 0.1) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
        min_length: æœ€å°é•¿åº¦
        min_chinese_ratio: æœ€å°ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        
    Returns:
        True è¡¨ç¤ºæ–‡æœ¬æœ‰æ•ˆï¼ŒFalse è¡¨ç¤ºæ— æ•ˆï¼ˆå¯èƒ½æ˜¯ä¹±ç æˆ–å¤ªçŸ­ï¼‰
    """
    if not text or len(text.strip()) < min_length:
        return False
        
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    ratio = chinese_chars / len(text)
    
    return ratio >= min_chinese_ratio


def process_pdf(pdf_path: Path) -> Dict:
    """
    å¤„ç†å•ä¸ª PDF æ–‡ä»¶
    å…ˆç”¨ pypdfï¼Œå¦‚æœæ–‡æœ¬å¤ªå°‘æˆ–ä¹±ç å†ç”¨ unstructured
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
    
    Returns:
        åŒ…å« text å’Œ title çš„å­—å…¸
    """
    print(f"å¤„ç†: {pdf_path.name}")
    
    # å…ˆå°è¯• pypdf
    text = extract_with_pypdf(pdf_path)
    
    # æ£€æŸ¥æ–‡æœ¬è´¨é‡
    if not is_valid_text(text):
        print(f"  pypdf æå–æ–‡æœ¬è´¨é‡ä¸ä½³ï¼ˆé•¿åº¦={len(text)}ï¼‰ï¼Œå°è¯• unstructured...")
        text = extract_with_unstructured(pdf_path)
    
    if not text or len(text.strip()) < 50:
        print(f"  âš ï¸  æœªæå–åˆ°è¶³å¤Ÿæ–‡æœ¬")
        return None
    
    # æ¸…ç†æ–‡æœ¬
    text = clean_text(text)
    
    # ä½¿ç”¨æ–‡ä»¶åä½œä¸º title
    title = pdf_path.stem
    
    print(f"  âœ“ å·²æå– {len(text)} å­—ç¬¦")
    
    return {
        "text": text,
        "title": title
    }


def clean_text(text: str) -> str:
    """æ¸…ç†æå–çš„æ–‡æœ¬"""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # ç§»é™¤æ±‰å­—ä¹‹é—´çš„ç©ºæ ¼
    # åŒ¹é…æ¨¡å¼ï¼šæ±‰å­— + ç©ºæ ¼ + æ±‰å­—
    # ä½¿ç”¨ lookbehind å’Œ lookahead ç¡®ä¿åªåˆ é™¤æ±‰å­—é—´çš„ç©ºæ ¼
    text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
    
    return text.strip()


def find_pdfs(src_dir: Path, recursive: bool = True) -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰ PDF æ–‡ä»¶"""
    if recursive:
        return sorted(src_dir.rglob("*.pdf"))
    else:
        return sorted(src_dir.glob("*.pdf"))


def main():
    # ç›´æ¥åœ¨ä»£ç ä¸­è®¾ç½®å‚æ•°
    src_dir = "data/raw/2025çœå¸‚æ–‡ä»¶_æœ‰è¡¨æ ¼"
    output = "data/processed/medical_docs.json"
    recursive = True
    max_files = None  # è®¾ç½®ä¸º None è¡¨ç¤ºå¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œæˆ–è®¾ç½®å…·ä½“æ•°å­—é™åˆ¶å¤„ç†æ•°é‡
    
    # æ£€æŸ¥æºç›®å½•
    src_dir_path = Path(src_dir)
    if not src_dir_path.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {src_dir_path}")
        sys.exit(1)
    
    # æŸ¥æ‰¾ PDF æ–‡ä»¶
    print(f"\nğŸ“‚ æ‰«æç›®å½•: {src_dir_path}")
    pdf_files = find_pdfs(src_dir_path, recursive=recursive)
    
    if not pdf_files:
        print("âŒ æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        sys.exit(1)
    
    if max_files:
        pdf_files = pdf_files[:max_files]
    
    print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶\n")
    
    # å¤„ç†æ‰€æœ‰ PDF
    documents = []
    doc_id = 0
    
    for pdf_path in pdf_files:
        result = process_pdf(pdf_path)
        
        if result:
            result['id'] = doc_id
            documents.append(result)
            doc_id += 1
            print()
        else:
            print()
    
    # ä¿å­˜ç»“æœ (JSON)
    output_path = Path(output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with output_path.open('w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2)
        
    # ä¿å­˜ç»“æœ (CSV)
    import csv
    csv_path = output_path.with_suffix('.csv')
    with csv_path.open('w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'title', 'text'])
        for doc in documents:
            writer.writerow([doc.get('id', ''), doc.get('title', ''), doc.get('text', '')])
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"å¤„ç†äº† {len(documents)}/{len(pdf_files)} ä¸ªæ–‡ä»¶")
    print(f"è¾“å‡ºæ–‡ä»¶ (JSON): {output_path}")
    print(f"è¾“å‡ºæ–‡ä»¶ (CSV): {csv_path}")
    print(f"æ€»è®¡: {sum(len(doc['text']) for doc in documents)} å­—ç¬¦")


if __name__ == "__main__":
    main()
